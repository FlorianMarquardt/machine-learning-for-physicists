{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FlorianMarquardt/machine-learning-for-physicists/blob/master/2024/05_tutorial_tSNE_tutorial_jax.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction via t-SNE\n",
    "\n",
    "Example code for the lecture series \"Machine Learning for Physicists\" by Florian Marquardt\n",
    "\n",
    "Lecture 5, Tutorial (this is discussed in session 5)\n",
    "\n",
    "See https://machine-learning-for-physicists.org and the current course website linked there!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following python code for t-SNE is an original code by the inventor of t-SNE, Laurens van der Maaten. It is available on his website https://lvdmaaten.github.io/tsne/ . \n",
    "\n",
    "It is stated on that website: \"*You are free to use, modify, or redistribute this software in any way you want, but only for non-commercial purposes. The use of the software is at your own risk; the authors are not responsible for any damage as a result from errors in the software.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T15:17:48.704427Z",
     "iopub.status.busy": "2021-05-17T15:17:48.704000Z",
     "iopub.status.idle": "2021-05-17T15:17:49.228057Z",
     "shell.execute_reply": "2021-05-17T15:17:49.226945Z",
     "shell.execute_reply.started": "2021-05-17T15:17:48.704365Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.dpi']=300 # highres display\n",
    "\n",
    "# for updating display \n",
    "# (very simple animation)\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import jax \n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T15:17:49.868883Z",
     "iopub.status.busy": "2021-05-17T15:17:49.868539Z",
     "iopub.status.idle": "2021-05-17T15:17:50.866683Z",
     "shell.execute_reply": "2021-05-17T15:17:50.865882Z",
     "shell.execute_reply.started": "2021-05-17T15:17:49.868824Z"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "#  tsne.py\n",
    "#\n",
    "# Implementation of t-SNE in Python. The implementation was tested on Python\n",
    "# 2.7.10, and it requires a working installation of NumPy. The implementation\n",
    "# comes with an example on the MNIST dataset. In order to plot the\n",
    "# results of this example, a working installation of matplotlib is required.\n",
    "#\n",
    "# The example can be run by executing: `ipython tsne.py`\n",
    "#\n",
    "#\n",
    "#  Created by Laurens van der Maaten on 20-12-08.\n",
    "#  Copyright (c) 2008 Tilburg University. All rights reserved.\n",
    "\n",
    "# note by FM: For this notebook, the MNIST example was dropped\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the t-SNE\n",
    "\n",
    "In this exercise you do some mathematical derivation for t-SNE. \n",
    "\n",
    "t-SNE is a non-linear method to compress and classify hidimension data. Assume we have a set of data $\\{X_i\\}_{i=1}^N$ with very high dimension. For example, pixels of images of human expressions. One woiuld like to extrack key informations from this complex data set. For example, tell the mood according to the expression shown on the image. \n",
    "\n",
    "The process of t-SNE is shown below: \n",
    "\n",
    "## Step 1:\n",
    "We construct a probability distribution which shows the \"distance\" between different data. First we have:\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    S_{j|i} &= \\exp (- \\beta_i ||X_i - X_j||^2) = \\exp (- \\beta_i D_{ij}) \\\\\n",
    "    P_{j|i} &= \\frac{S_{j|i}}{\\sum_k S_{k|i} } \n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "and set $p_{i|i}=0$. \n",
    "Then the probability distribution $P$ is set as: \n",
    "\\begin{equation}\n",
    "P_{ij} = \\frac{(S_{j|i} + S_{i|j})}{2 \\sum_{k,l} S_{k|l} + S_{l|k}}\n",
    "\\end{equation}\n",
    "\n",
    "The value of $\\beta_i$ are chosen with a bicestion searching such that $P_{j|i}$ have the same conditional entropy.\n",
    "\n",
    "## Step 2:\n",
    "For arbitrary low dimension data $\\{Y_i\\}_{i=1}^N$, we define the probability distribution $Q$ as:\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    R_{ij} &= \\frac{1}{1 + ||Y_i - Y_j||^2} \\\\\n",
    "    Q_{ij} &= \\frac{R_{ij}}{\\sum_{k,l} R_{kl}}\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "## Step 3: \n",
    "We view $Y_i$ as the trainable parameter. We find the optimal $Y_i$ such that the distance between $P$ and $Q$ (measured with Kullback-Leiber divergence) is as small as possible:\n",
    "\\begin{equation}\n",
    "Y^*_i = \\argmax_{Y_i} \\sum_{i \\neq j} P_{ij} \\log(\\frac{P_{ij}}{Q_{ij}})\n",
    "\\end{equation}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex.1\n",
    "In this exercise you are supposed to finish the function which implement the construction of $P_{ij}$ described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Given a set of data, get the difference matrix D_ij = \\sum_k (x_ik - x_jk)^2 and\n",
    "calculate the distribution\n",
    "'''\n",
    "\n",
    "@jax.jit\n",
    "def get_D(X):\n",
    "    ###########################################################\n",
    "    # Finish the code with which you get the differenc matrix #\n",
    "    #            D_ij = \\sum_k (x_ik - x_jk)^2                #\n",
    "    ###########################################################\n",
    "    #------------------------------------------\n",
    "\n",
    "    #------------------------------------------\n",
    "    return D\n",
    "\n",
    "@jax.jit\n",
    "def Hbeta(D=jnp.asarray([]), beta=1.0):\n",
    "    '''\n",
    "    From WQS: Assume you know D_ij, take the i-th row of D_ij as the input (D), output \n",
    "    p_j|i = exp(-beta * D_ij)/(\\sum_j exp(-beta * D_ij)) and its entropy. \n",
    "    '''\n",
    "    \n",
    "    \"\"\"\n",
    "        Compute the perplexity and the P-row for a specific value of the\n",
    "        precision of a Gaussian distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute P-row and corresponding perplexity\n",
    "    ###################\n",
    "    # Finish the code #\n",
    "    ###################\n",
    "    \n",
    "    #------------------------------------------\n",
    "\n",
    "    #------------------------------------------\n",
    "    return H, P"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following cell you can plot the cureve of entropy.\n",
    "\n",
    "If everything is implemented correctly, you will find that the conditional entropy is a decreasing function in beta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From WQS: Verify that H in Hbeta is always a dicreasing function in beta\n",
    "D = np.power(np.random.randn(1000),2)\n",
    "beta = 0.1\n",
    "beta_list = np.arange(0, 100, 0.01)\n",
    "H_list = []\n",
    "for k in range(0, beta_list.shape[0]):\n",
    "    H_list.append(Hbeta(D, beta_list[k])[0])\n",
    "\n",
    "plt.plot(beta_list, np.asarray(H_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex.2:\n",
    "In this exercise, you are supposed to implement the construction of $P_{ij}$. The key is to find approriate $\\beta$ for each pieces of data with a bisection search.\n",
    "\n",
    "See https://en.wikipedia.org/wiki/Bisection_method for the concept of bisection search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jax.jit\n",
    "def jax_bisection_search_beta(Di, beta, logU, tol, try_lim=50, betamin=0, betamax=1e6):\n",
    "    '''\n",
    "    Di: the i-th row of D-matrix. Used as the input.\n",
    "    beta: the beta used for Di.\n",
    "    logU: the target entropy.\n",
    "    tol: tolerance for beta\n",
    "    try_lim: upper limit of times of trial search\n",
    "    '''\n",
    "    \n",
    "    ###################\n",
    "    # Finish the Code #\n",
    "    ###################\n",
    "    \n",
    "    #------------------------------------------\n",
    "    # Set the initial state \n",
    "    \n",
    "    # Evaluate whether the perplexity is within tolerance\n",
    "\n",
    "    #------------------------------------------\n",
    "    return thisP, beta\n",
    "\n",
    "\n",
    "#@jax.jit\n",
    "def x2p(X=np.array([]), tol=1e-5, perplexity=30.0):\n",
    "    \"\"\"\n",
    "        Performs a bisection search to get P-values in such a way that each\n",
    "        conditional Gaussian has the same perplexity.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize some variables\n",
    "    print(\"Computing pairwise distances...\")\n",
    "    logU = jnp.log(perplexity) # Target value of conditional entropy\n",
    "    (n, d) = X.shape\n",
    "    sum_X = jnp.sum(jnp.square(X), 1)\n",
    "    P = jnp.zeros((n, n))\n",
    "    beta = jnp.ones(n)\n",
    "    # Get D_ij = \\sum_k (x_ik - x_jk)^2\n",
    "    \n",
    "    #------------------------------------------\n",
    "    # Get D matrix\n",
    "    print(\"Computing D\")\n",
    "\n",
    "    # Loop over all datapoints\n",
    "    # From WQS: get the bandwidth of Gaussian kernels \\beta with bisection searching\n",
    "    \n",
    "    print(\"search for beta\")\n",
    "    \n",
    "    print(\"Mean value of sigma: %f\" % jnp.mean(jnp.sqrt(1 / beta)))\n",
    "    #------------------------------------------\n",
    "    \n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(10,100)\n",
    "P = x2p(X, perplexity=3)\n",
    "print(P)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex.3\n",
    "In this exercise we finish the whole process of t-SNE.\n",
    "\n",
    "As described above, we need a function to calculate $R_{ij}$ and $Q_{ij}$ (set $R_{ii} = 0$):\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    R_{ij} &= \\frac{1}{1 + ||Y_i - Y_j||^2} \\\\\n",
    "    Q_{ij} &= \\frac{R_{ij}}{\\sum_{k,l} R_{kl}}\n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "You are supposed to do: \n",
    "\n",
    "## Ex.3.1\n",
    "Finish function \"y2Q\".\n",
    "\n",
    "## Ex.3.2\n",
    "\n",
    "Derive the gradient of probability distance over $Y_i$. Show that we define $L$ as: \n",
    "\n",
    "\\begin{equation}\n",
    "L(Y) = \\sum_{i \\neq j} P_{ij} \\log(\\frac{P_{ij}}{Q_{ij}(Y)})\n",
    "\\end{equation}\n",
    "\n",
    "and the process of t-SNE is to find optimal $Y$ through gradient descent:\n",
    "\\begin{equation}\n",
    "Y^* = \\argmax_{Y} L(Y)\n",
    "\\end{equation}\n",
    "\n",
    "Show that the gradient is ($Y_{ik}$ is the k-th component of $Y_i$):\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial Y_{ik}} = 4\\sum_n (P_{in} - Q_{in}) Q_{in} (Y_{ik} - Y_{nk})\n",
    "\\end{equation}\n",
    "\n",
    "## Ex.3.3\n",
    "Based on the equation above, finish function \"KLD_and_grad_Y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X=np.array([]), no_dims=50):\n",
    "    \"\"\"\n",
    "        Runs PCA on the NxD array X in order to reduce its dimensionality to\n",
    "        no_dims dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Preprocessing the data using PCA...\")\n",
    "    (n, d) = X.shape\n",
    "    X = X - jnp.tile(jnp.mean(X, 0), (n, 1))\n",
    "    (l, M) = jnp.linalg.eig(jnp.dot(X.T, X))\n",
    "    Y = jnp.dot(X, M[:, 0:no_dims])\n",
    "    return Y\n",
    "\n",
    "@jax.jit\n",
    "def y2Q(Y):\n",
    "    \n",
    "    ######################################\n",
    "    #          Finish the code.          #\n",
    "    # This return R matrix and Q matrix. #\n",
    "    ######################################\n",
    "    \n",
    "    #------------------------------------------\n",
    "    \n",
    "    #------------------------------------------\n",
    "    \n",
    "    return R, Q\n",
    "\n",
    "@jax.jit\n",
    "def KLD_and_grad_Y(P, Y):\n",
    "    \n",
    "    ##############################################\n",
    "    #      Finish the code. This return the      #\n",
    "    # KL divergence and the gradient of L over Y #\n",
    "    ##############################################\n",
    "    \n",
    "    #------------------------------------------\n",
    "    \n",
    "    \n",
    "    #----------------------------------------------\n",
    "    return C, dY\n",
    "\n",
    "def tsne(X=jnp.array([]), no_dims=2, initial_dims=50, perplexity=30.0, \n",
    "         do_animation=False, animation_skip_steps=10, max_iter = 1000):\n",
    "    \"\"\"\n",
    "        Runs t-SNE on the dataset in the NxD array X to reduce its\n",
    "        dimensionality to no_dims dimensions. The syntaxis of the function is\n",
    "        `Y = tsne.tsne(X, no_dims, perplexity), where X is an NxD NumPy array.\n",
    "        \n",
    "        Added by F. Marquardt: do_animation==True will give you a graphical animation of\n",
    "        the progress, use animation_skip_steps to control how often this will\n",
    "        be plotted; max_iter controls the total number of gradient descent steps\n",
    "    \"\"\"\n",
    "\n",
    "    # Check inputs\n",
    "    if isinstance(no_dims, float):\n",
    "        print(\"Error: array X should have type float.\")\n",
    "        return -1\n",
    "    if round(no_dims) != no_dims:\n",
    "        print(\"Error: number of dimensions should be an integer.\")\n",
    "        return -1\n",
    "\n",
    "    # Initialize variables\n",
    "    # By WQS: use preprocess the data.\n",
    "    # Preprocess the data with PCA\n",
    "    #-----------------------------\n",
    "    \n",
    "    #-----------------------------\n",
    "    (n, d) = X.shape\n",
    "    # By WQS: set initial parameters\n",
    "    eta = 500\n",
    "    Y = jnp.asarray(np.random.randn(n, no_dims)) # WQS: Output\n",
    "    iY = 0.* Y\n",
    "\n",
    "    # Compute P-values\n",
    "    #----------------------------\n",
    "    \n",
    "    #----------------------------\n",
    "    #P = P * 4.\t\t\t\t\t\t\t\t\t# early exaggeration\n",
    "    P = jnp.maximum(P, 1e-12)\n",
    "    print(P)\n",
    "\n",
    "    if do_animation: # added by FM\n",
    "        costs=jnp.zeros(max_iter) # to store the cost values\n",
    "        \n",
    "    # Run iterations\n",
    "    # WQS: run iterations for gradient descent\n",
    "    print(\"tsne start...\")\n",
    "    for iter in range(max_iter):\n",
    "        \n",
    "        ##############################################\n",
    "        # Completet the process of gradient descent. #\n",
    "        ##############################################\n",
    "        \n",
    "        #-----------------------------------------------\n",
    "        \n",
    "        #-----------------------------------------------\n",
    "\n",
    "        # These lines below plot the process of training with animation\n",
    "        if not do_animation: # added by FM: do not print if we are animating!\n",
    "            # Compute current value of cost function\n",
    "            if (iter + 1) % 10 == 0:\n",
    "                print(\"Iteration %d: error is %f\" % (iter + 1, C), end=\"           \\r\") # modified to overwrite line\n",
    "\n",
    "        # Stop lying about P-values\n",
    "        if iter == 100:\n",
    "            #P = P / 4.\n",
    "            pass\n",
    "            \n",
    "        if do_animation:  # added by FM\n",
    "            costs = costs.at[iter].set(C)\n",
    "            if iter % animation_skip_steps==0:\n",
    "                clear_output(wait=True)\n",
    "                fig,ax=plt.subplots(ncols=2,nrows=1,figsize=(10,5))\n",
    "                ax[0].plot(costs)\n",
    "                ax[1].scatter(Y[:,0],Y[:,1],color=\"orange\")\n",
    "                plt.show()\n",
    "\n",
    "    # Return solution\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: A few Gaussian clouds in high-dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-17T15:18:05.574894Z",
     "iopub.status.busy": "2021-05-17T15:18:05.574549Z",
     "iopub.status.idle": "2021-05-17T15:18:05.926544Z",
     "shell.execute_reply": "2021-05-17T15:18:05.925795Z",
     "shell.execute_reply.started": "2021-05-17T15:18:05.574833Z"
    }
   },
   "outputs": [],
   "source": [
    "# produce a high-dimensional data set, composed of a few Gaussian point clouds in high-dimensional space\n",
    "\n",
    "n_dim=100 # a really high-dimensional space\n",
    "\n",
    "n_clusters=5 # number of clusters, i.e. clouds\n",
    "N_cluster_points=30 # number of points inside each cluster\n",
    "N=n_clusters*N_cluster_points # total number of points\n",
    "\n",
    "Gauss_spread=1.0 # size of each cluster (cloud)\n",
    "\n",
    "X=np.zeros([N,n_dim])\n",
    "\n",
    "for j in range(n_clusters):\n",
    "    Xmean=np.random.randn(n_dim) # the center position of the cluster\n",
    "    X[j*N_cluster_points:(j+1)*N_cluster_points,:]=Xmean[None,:]+Gauss_spread*np.random.randn(N_cluster_points,n_dim)\n",
    "\n",
    "# plot these points in a projection into 2D space\n",
    "# color them according to the cluster they belong to!\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "for j in range(n_clusters):\n",
    "    X0=X[j*N_cluster_points:(j+1)*N_cluster_points,0]\n",
    "    X1=X[j*N_cluster_points:(j+1)*N_cluster_points,1]    \n",
    "    plt.scatter(X0,X1,label='class {0}'.format(j))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now apply t-SNE to reduce to two dimensions in a smart way!\n",
    "Y=tsne(X, no_dims=2, initial_dims=50, perplexity=20.0, \n",
    "       do_animation=True, animation_skip_steps=10, max_iter=300)\n",
    "\n",
    "# plot the points according to the t-SNE projection into 2D space\n",
    "# color them according to the cluster they belong to!\n",
    "plt.figure(figsize=(8,8))\n",
    "for j in range(n_clusters):\n",
    "    Y0=Y[j*N_cluster_points:(j+1)*N_cluster_points,0]\n",
    "    Y1=Y[j*N_cluster_points:(j+1)*N_cluster_points,1]    \n",
    "    plt.scatter(Y0,Y1,label='class {0}'.format(j))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Multiple Gaussians: finding the true number without labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a tSNE-scatterplot, but with some randomly marked points,\n",
    "# together with the corresponding high-dimensional data points plotted\n",
    "# as curves!\n",
    "\n",
    "def plot_tsne_with_curves(y0,y1,x,xlabel,n_picks=10,vmin=-0.1,vmax=2):\n",
    "    random_picks=np.random.randint(np.shape(y0)[0],size=n_picks) # pick some random points\n",
    "    \n",
    "    fig=plt.figure(constrained_layout=True,figsize=(8,4))\n",
    "    gs=fig.add_gridspec(ncols=8,nrows=4)\n",
    "    scatter_plot=fig.add_subplot(gs[0:4,0:4])\n",
    "    \n",
    "    myplot={}\n",
    "    j=0\n",
    "    for n0 in range(4):\n",
    "        for n1 in range(4):\n",
    "            myplot[j]=fig.add_subplot(gs[n0,4+n1])\n",
    "            myplot[j].axis('off')\n",
    "            j+=1\n",
    "    \n",
    "    scatter_plot.scatter(y0,y1,c=xlabel)\n",
    "    scatter_plot.scatter(y0[random_picks],y1[random_picks],color=\"black\",alpha=0.7,s=80)\n",
    "    \n",
    "    for idx in range(len(random_picks)):\n",
    "        scatter_plot.text(y0[random_picks[idx]], y1[random_picks[idx]], \n",
    "                      str(idx), fontsize=8, color=\"orange\", \n",
    "                     alpha=0.8, horizontalalignment='center',\n",
    "                verticalalignment='center')\n",
    "    \n",
    "    for idx,m in enumerate(random_picks):\n",
    "        if idx<j:\n",
    "            myplot[idx].plot(x[m,:])\n",
    "            myplot[idx].text(0.1, 0.75, str(idx), fontsize=12, color=\"orange\", \n",
    "                             alpha=0.5, horizontalalignment='center',\n",
    "                        verticalalignment='center', transform=myplot[idx].transAxes)\n",
    "            myplot[idx].set_ylim([vmin,vmax])\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_generator1D(batchsize,x): # produce a batch of curves, a random number of Gaussian\n",
    "    maxNum=2 # the maximum number of Gaussians\n",
    "    NumGaussians=np.random.randint(maxNum+1,size=batchsize) # select the number, for each sample\n",
    "    Curves=np.zeros([batchsize,len(x)])\n",
    "    for j in range(maxNum):\n",
    "        R=np.random.uniform(low=0.1,high=0.11,size=batchsize) # width\n",
    "        A=np.random.uniform(low=0.9,high=1.0,size=batchsize) # amplitude\n",
    "        x0=np.random.uniform(size=batchsize,low=-0.8,high=0.8) # position\n",
    "        Curves+=(j<=NumGaussians[:,None]-1)*A[:,None]*np.exp(-((x[None,:]-x0[:,None])/R[:,None])**2)\n",
    "    Curves+=0.1*np.random.randn(batchsize,len(x)) # add a bit of background noise on top\n",
    "    return( Curves, NumGaussians )\n",
    "\n",
    "n_dim=100\n",
    "x=np.linspace(-1,1,n_dim)\n",
    "N=2000 # how many curves\n",
    "\n",
    "X,Xlabel=my_generator1D(N,x) \n",
    "# small x is the coordinate, capital X are the high-dim. \"data points\", meaning samples of curves!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols=10,nrows=1,figsize=(10,1))\n",
    "for n in range(10):\n",
    "    ax[n].plot(X[n,:])\n",
    "    ax[n].set_ylim([-0.1,2])\n",
    "    ax[n].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot these points in a projection into 2D space\n",
    "\n",
    "# pick two arbitrary coordinates\n",
    "j0=17\n",
    "j1=35\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "X0=X[:,j0]\n",
    "X1=X[:,j1]    \n",
    "plt.scatter(X0,X1,c=Xlabel)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now apply t-SNE to reduce to two dimensions in a smart way!\n",
    "Y=tsne(X, 2, 20, 30.0, do_animation=True, animation_skip_steps=10, max_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne_with_curves(Y[:,0],Y[:,1],X,Xlabel,n_picks=16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Exercise 2: Change the perplexity parameter in the 'Gaussian cloud' example and/or the 'multiple Gaussians' example and observe its effect!\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Exercise 3: Modify the 'multiple Gaussians' example by inventing different sample curves\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
